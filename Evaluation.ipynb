{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain\n",
        "!pip install -q ragas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBDHJ8Ppjb59",
        "outputId": "1f51076a-7173-4d44-e127-c54e90f467bd"
      },
      "id": "kBDHJ8Ppjb59",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.8/48.8 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.8/220.8 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0174eb96",
      "metadata": {
        "id": "0174eb96"
      },
      "source": [
        "# GPT evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55f0f9b9",
      "metadata": {
        "id": "55f0f9b9"
      },
      "source": [
        "## Evaluating with GPT4\n",
        "\n",
        "Ragas uses gpt3.5 by default but using gpt4 for evaluation can improve the results so lets use that for the `Faithfulness` metric\n",
        "\n",
        "To start-off, we initialise the gpt4 `chat_model` from langchain and ada embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a6d96660",
      "metadata": {
        "id": "a6d96660"
      },
      "outputs": [],
      "source": [
        "# make sure you have you OpenAI API key ready\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"INSERT_OPENAI_KEY_HERE\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "6906a4d6",
      "metadata": {
        "id": "6906a4d6"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "gpt4 = ChatOpenAI(model_name=\"gpt-4\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "25c72521-3372-4663-81e4-c159b0edde40",
      "metadata": {
        "id": "25c72521-3372-4663-81e4-c159b0edde40"
      },
      "outputs": [],
      "source": [
        "from ragas.llms import LangchainLLM\n",
        "\n",
        "gpt4_wrapper = LangchainLLM(llm=gpt4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62645da8-6a52-4cbb-bec7-59f7e153cd38",
      "metadata": {
        "id": "62645da8-6a52-4cbb-bec7-59f7e153cd38"
      },
      "source": [
        "Substitute the llm in `Metric` instance with the newly create GPT4 model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "307321ed",
      "metadata": {
        "id": "307321ed"
      },
      "outputs": [],
      "source": [
        "from ragas.metrics import faithfulness\n",
        "\n",
        "faithfulness.llm = gpt4_wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "62c0eadb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62c0eadb",
        "outputId": "cdfcd00c-d98e-491d-82f3-34df45709575"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    baseline: Dataset({\n",
              "        features: ['question', 'ground_truths', 'answer', 'contexts'],\n",
              "        num_rows: 30\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# data\n",
        "from datasets import load_dataset\n",
        "\n",
        "fiqa_eval = load_dataset(\"explodinggradients/fiqa\", \"ragas_eval\")\n",
        "fiqa_eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "c4396f6e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4396f6e",
        "outputId": "0d6bc1a8-5915-47ca-cc4e-4d8d90ea0c9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "evaluating with [faithfulness]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [02:58<00:00, 178.61s/it]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'faithfulness': 0.9500}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# evaluate\n",
        "from ragas import evaluate\n",
        "\n",
        "result = evaluate(\n",
        "    fiqa_eval[\"baseline\"].select(range(5)),\n",
        "    metrics=[faithfulness],\n",
        ")\n",
        "\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f490031e-fb73-4170-8762-61cadb4031e6",
      "metadata": {
        "id": "f490031e-fb73-4170-8762-61cadb4031e6"
      },
      "source": [
        "## Evaluating with Open-Source LLMs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85e313f2-e45c-4551-ab20-4e526e098740",
      "metadata": {
        "id": "85e313f2-e45c-4551-ab20-4e526e098740"
      },
      "outputs": [],
      "source": [
        "# start the vLLM server\n",
        "!python -m vllm.entrypoints.openai.api_server \\\n",
        "    --model HuggingFaceH4/zephyr-7b-alpha \\\n",
        "    --host 0.0.0.0 \\\n",
        "    --port 8080"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fd4adf3-db15-4c95-bf7c-407266517214",
      "metadata": {
        "id": "2fd4adf3-db15-4c95-bf7c-407266517214"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from ragas.llms import LangchainLLM\n",
        "\n",
        "inference_server_url = \"http://localhost:8080/v1\"\n",
        "\n",
        "# create vLLM Langchain instance\n",
        "chat = ChatOpenAI(\n",
        "    model=\"HuggingFaceH4/zephyr-7b-alpha\",\n",
        "    openai_api_key=\"no-key\",\n",
        "    openai_api_base=inference_server_url,\n",
        "    max_tokens=5,\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "# use the Ragas LangchainLLM wrapper to create a RagasLLM instance\n",
        "vllm = LangchainLLM(llm=chat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20882d05-1b54-4d17-88a0-f7ada2d6a576",
      "metadata": {
        "id": "20882d05-1b54-4d17-88a0-f7ada2d6a576"
      },
      "outputs": [],
      "source": [
        "from ragas.metrics import (\n",
        "    context_precision,\n",
        "    answer_relevancy,\n",
        "    faithfulness,\n",
        "    context_recall,\n",
        ")\n",
        "from ragas.metrics.critique import harmfulness\n",
        "\n",
        "# change the LLM\n",
        "\n",
        "faithfulness.llm = vllm\n",
        "answer_relevancy.llm = vllm\n",
        "context_precision.llm = vllm\n",
        "context_recall.llm = vllm\n",
        "harmfulness.llm = vllm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8858300-7985-4c79-8d03-c671afd645ac",
      "metadata": {
        "id": "d8858300-7985-4c79-8d03-c671afd645ac"
      },
      "outputs": [],
      "source": [
        "# evaluate\n",
        "from ragas import evaluate\n",
        "\n",
        "result = evaluate(\n",
        "    fiqa_eval[\"baseline\"].select(range(5)),  # showing only 5 for demonstration\n",
        "    metrics=[faithfulness],\n",
        ")\n",
        "\n",
        "result"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}